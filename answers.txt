# Technical Challenge 

## Step 1 (Data Analysis):

### Do you think there is bot traffic on this site? Why?
Yes for some many reasons. First, we can see there are a lot of logs from the same remote host in a short time. Then, we can also see in the user-agent some very famomus bots 
(like google bot, bing bot, AhrefsBot, etc).
Also, if we check the robots.txt file of the website, we can see there are some bots trying to enter into the forbidden folders.

### How would you qualify a "normal" human traffic on this site? Based on which criteria?

With the website log of this site, I would detect the number of requests. If you can see a large number of request in a short time, it is not a normal human traffic, therefore is a bot.

Also, "if a domain is benign, even if is is not very popular, the number of queries should exceed the threshold at least several times during the monitoring period."

A normal human traffic won't show daily similarities in their request count change over time and will not regularly repeat patterns. And the number the number of IP addresses resolved for a given domain during defined time window shouldn't be so big.

https://geant3.archive.geant.org/media_centre/media_library/media%20library/gn3_jra2_t4_m4_deliverable.pdf 

### Could you focus on 1 good bot and 1 bad bot to tell us more about them?
One good bot is AhrefBots (marketing bot) which, according to their website, it crawls the website making notes of outbound links and adding them to the ahrefbots database. It will periodically re-crawl the website to check the current status of previously found links. It does not collect or store any other information about the website. It does not trigger ads on your website (if any) and wonâ€™t add numbers to your Google Analytics traffic.
Also, the good bots will always respects the robots.txt file

A type of bad bot is a Mail address harvesting bot, that is a spider visiting the site harvesting mailto: mail addresses to send spam later. One example is the spam mail address harvesting bot, hosted by GWBN XIAMEN QIANPU broadband access company gwbn.net.cn, CN; later sending Nigerian 419-scam spam via bigpond.com. (http://www.kloth.net/internet/badbots-2003.php)


##Step 2 --> AccessLog.java
Implemented in the AccessLog java file and Main java file.

##Step 3 --> dnsLookUp.java
To make faster the reverse DNS lookup, I implemented a binary search algorithm to search the ip address in the white and black list. This way, we will search faster (O(logN) on the worst case)
and we will be able to detect the bots.

One limitation is the number of addresses (i.e. hostnames). We should reduce this number to consume the log as fast as possible. So, in order to do that, we could  determine if each request is really necessary. (https://kinsta.com/blog/reduce-dns-lookups/)

We could also change the TTL value. The higher the TTL, the less the browser will need to perform another DNS lookup. 

One problem is how can we save the white and black list, and when remove some (or all) the ip addresses

I tried to implement a binary search in the dns lookup algorithm (procedureDNS), but it was not faster than the original algorithm (procedureDNSimprove)

I would implement a better binary search or another search method to optimize the time